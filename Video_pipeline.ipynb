{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Distortion Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "#Load calibrated camera\n",
    "Cam_Calib=pickle.load( open(\"CamCal/camCal_pickle.p\", \"rb\"))\n",
    "\n",
    "save_dir= r'output_images/pipeline/'\n",
    "\n",
    "cam_dist=Cam_Calib[\"dist\"]\n",
    "cam_mtx=Cam_Calib[\"mtx\"]\n",
    "\n",
    "def undistort_s(img, dist=cam_dist, mtx=cam_mtx):\n",
    "    '''Undistorts color image, ouputs undistorted img, saturation channel'''\n",
    "    dst=cv2.undistort(img, cam_mtx, cam_dist, None, cam_mtx)\n",
    "    HLS_img=cv2.cvtColor(dst, cv2.COLOR_RGB2HLS)\n",
    "    S_channel= HLS_img[:,:,2]\n",
    "    return dst, S_channel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Gradient Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Diagonal sobel kernels from http://homepages.inf.ed.ac.uk/rbf/HIPR2/linedet.htm\n",
    "up_kernel = [[-1.,-1.,2.],\n",
    "         [-1.,2.,-1.],\n",
    "         [2.,-1.,-1.]]\n",
    "up_kernel=np.asarray(up_kernel)/12.0\n",
    "\n",
    "down_kernel = [[2.,-1.,-1.],\n",
    "         [-1.,2.,-1.],\n",
    "         [-1.,-1.,2.]]\n",
    "down_kernel=np.asarray(down_kernel)/12.0\n",
    "\n",
    "def abs_sobel_thresh(gray, orient='x', thresh_min=0, thresh_max=255, sobel_kernel=3):\n",
    "    \"\"\"Axis sobel, asumes img is gray image, returns binary image\"\"\"\n",
    "    # Apply the following steps to img\n",
    "    # 2) Take the derivative in x or y given orient = 'x' or 'y'\n",
    "    if orient=='x':\n",
    "        sobel = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    elif orient=='y':\n",
    "        sobel = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    elif orient=='+':\n",
    "        sobel = cv2.filter2D(gray,-1,up_kernel)\n",
    "    elif orient=='-':\n",
    "        sobel = cv2.filter2D(gray,-1,down_kernel)\n",
    "    else:\n",
    "        raise \"Not valid orientation!\"\n",
    "    # 3) Take the absolute value of the derivative or gradient\n",
    "    abs_sobel = np.absolute(sobel)\n",
    "    # 4) Scale to 8-bit (0 - 255) then convert to type = np.uint8\n",
    "    scaled_sobel=np.uint8(255 * abs_sobel/np.max(abs_sobel))\n",
    "    # 5) Create a mask of 1's where the scaled gradient magnitude \n",
    "    binary_output=np.zeros_like(scaled_sobel)\n",
    "            # is > thresh_min and < thresh_max\n",
    "    # 6) Return this mask as your binary_output image\n",
    "    binary_output[(scaled_sobel>thresh_min) & (scaled_sobel<thresh_max)] = 1\n",
    "    return binary_output\n",
    "\n",
    "def hls_tresh(S_channel, low_limit=0, high_limit=255):\n",
    "    \"\"\"Takes saturation channel, outputs binary image\"\"\"\n",
    "    s_binary=np.zeros_like(S_channel)\n",
    "    s_binary[(S_channel>=low_limit) & (S_channel<=high_limit)]=1\n",
    "    return s_binary\n",
    "\n",
    "def multiple_threshold(image, S_channel, min_dt=0, max_dt=255, s_low=128, s_high=255, k_slope1=up_kernel, k_slope2=down_kernel):\n",
    "    \"\"\"Takes gray image and Saturation channel, outputs binary image\"\"\"\n",
    "    # Apply the following steps to img\n",
    "    # 1) Apply sobel in direction\n",
    "    #X not used, noisy\n",
    "    #Less significant on Y axis , lots of noise\n",
    "    #Diagonal Sobels!\n",
    "    pos=abs_sobel_thresh(image, orient='+', thresh_min=min_dt, thresh_max=max_dt)\n",
    "    neg=abs_sobel_thresh(image, orient='-', thresh_min=min_dt, thresh_max=max_dt)\n",
    "    bin_diag=np.zeros_like(pos)\n",
    "    bin_diag[(pos==1)|(neg==1)]=1\n",
    "    #Close gaps in binary image\n",
    "    bin_diag=cv2.morphologyEx(bin_diag, cv2.MORPH_CLOSE, np.ones((7,3),np.uint8))\n",
    "#    \n",
    "    #Even smaller magnitudes cause noise, not using magnitude sobel\n",
    "    #Lots of noise (even when combining magnitude and orientation), not using orientation sobel\n",
    "    # 4) Saturation channel\n",
    "    s_bin= hls_tresh(S_channel ,low_limit=s_low, high_limit=s_high)\n",
    "    #Open big spaces, as to desaturate the saturated channel (bad_joke_here)\n",
    "    s_bin=cv2.morphologyEx(s_bin, cv2.MORPH_OPEN, np.ones((1,3),np.uint8))   \n",
    "    # 5)Mix 2 above\n",
    "    binary_output=np.zeros_like(bin_diag)\n",
    "    binary_output[((bin_diag==1)|(s_bin==1))]=1\n",
    "    #binary_output=bin_diag\n",
    "    binary_output= cv2.erode(binary_output,np.ones((1,3),np.uint8),iterations = 1)\n",
    "    return binary_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Prespective transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Polygon borders (Original IMG)\n",
    "src_pts = np.array([[230,700],[610,440],[670,440],[1080,700]], np.float32)\n",
    "#Destination Points\n",
    "dst_pts = np.array([[300,720],[300,-200],[950,-200],[950,720]], np.float32)\n",
    "\n",
    "def birds_eye(img, src=src_pts, dst=dst_pts):\n",
    "    \"\"\"Transform image to birdseye\"\"\"\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    bird = cv2.warpPerspective(img, M, (img.shape[1], img.shape[0]), flags=cv2.INTER_NEAREST)\n",
    "    return bird"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Line detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "window_width= 150\n",
    "window_height=20\n",
    "margin=40\n",
    "\n",
    "def window_mask(width, height, img_ref, center, level):\n",
    "    \"\"\"Get window mask\"\"\"\n",
    "    output = np.zeros_like(img_ref)\n",
    "    output[int(img_ref.shape[0]-(level+1)*height):int(img_ref.shape[0]-level*height),max(0,int(center-width/2)):min(int(center+width/2),img_ref.shape[1])] = 1\n",
    "    return output\n",
    "    \n",
    "def find_window_centroids(warped, window_width, window_height, margin, custom_window=None):\n",
    "    \"\"\"Find centroids of windows in image, returns windows centroids and central point of lanes\"\"\"\n",
    "    # Add here condition for custom distribution (not implemented)\n",
    "    window_centroids = [] # Store the (left,right) window centroid positions per level\n",
    "    window = np.ones(window_width) # Create our window template that we will use for convolutions\n",
    "    #Custom window, triangular shape as to push the weight of the convolution to the center\n",
    "    for i in range(int(len(window)/2)):\n",
    "        window[i]=window[i]+i\n",
    "        window[-i]=window[-i]+i\n",
    "    # First find the two starting positions for the left and right lane by using np.sum to get the vertical image slice\n",
    "    # and then np.convolve the vertical image slice with the window template \n",
    "    # Sum quarter bottom of image to get slice, could use a different ratio\n",
    "    l_sum = np.sum(warped[int(warped.shape[0]/4):,:int(warped.shape[1]/2)], axis=0)\n",
    "    l_center = np.argmax(np.convolve(window,l_sum))-window_width/2\n",
    "    r_sum = np.sum(warped[int(warped.shape[0]/4):,int(warped.shape[1]/2):], axis=0)\n",
    "    r_center = np.argmax(np.convolve(window,r_sum))-window_width/2+int(warped.shape[1]/2)\n",
    "    #Center of both lane markings\n",
    "    center=r_center-l_center\n",
    "    # Add what we found for the first layer\n",
    "    window_centroids.append((l_center,r_center))\n",
    "    \n",
    "    # Go through each layer looking for max pixel locations\n",
    "    for level in range(1,(int)(warped.shape[0]/window_height)):\n",
    "        # convolve the window into the vertical slice of the image\n",
    "        image_layer = np.sum(warped[int(warped.shape[0]-(level+1)*window_height):int(warped.shape[0]-level*window_height),:], axis=0)\n",
    "        conv_signal = np.convolve(window, image_layer, mode='full')\n",
    "        # Find the best left centroid by using past left center as a reference\n",
    "        # Use window_width/2 as offset because convolution signal reference is at right side of window, not center of window\n",
    "        offset = window_width/2\n",
    "        l_min_index = int(max(l_center+offset-margin,0))\n",
    "        l_max_index = int(min(l_center+offset+margin,warped.shape[1]))\n",
    "        max_conv = np.argmax(conv_signal[l_min_index:l_max_index])+l_min_index-offset\n",
    "        if conv_signal[int(max_conv)]>0:\n",
    "            l_center=max_conv\n",
    "        else:\n",
    "            l_center=l_center\n",
    "        # Find the best right centroid by using past right center as a reference\n",
    "        r_min_index = int(max(r_center+offset-margin,0))\n",
    "        r_max_index = int(min(r_center+offset+margin,warped.shape[1]))\n",
    "        max_conv = np.argmax(conv_signal[r_min_index:r_max_index])+r_min_index-offset\n",
    "        if conv_signal[int(max_conv)]>0:\n",
    "            r_center=max_conv\n",
    "        else:\n",
    "            r_center=r_center\n",
    "        # Add what we found for that layer\n",
    "        window_centroids.append((l_center,r_center))\n",
    "    return window_centroids, center\n",
    "\n",
    "def regression(window_centroids, window_height):\n",
    "    \"\"\"Get polinomial regression for left and right lanes\"\"\"\n",
    "    pt_l=[]\n",
    "    pt_r=[]\n",
    "    x_point=[]\n",
    "    for i, point in enumerate(window_centroids):\n",
    "        pt_l.append((point[0]))\n",
    "        pt_r.append((point[1]))\n",
    "        x_point.append(720-i*window_height)\n",
    "    fit_l=np.polyfit(x_point, pt_l, 2)\n",
    "    fit_r=np.polyfit(x_point, pt_r, 2)    \n",
    "    return fit_l, fit_r\n",
    "\n",
    "def draw_regression(warped, window_centroids, window_height):\n",
    "    \"\"\"Draws regression on image\"\"\"\n",
    "    if len(window_centroids) > 0:\n",
    "        #Get polinomials\n",
    "        left_fit, right_fit = regression(window_centroids, window_height)\n",
    "        # Points used to draw all the left and right windows\n",
    "        l_points = np.zeros_like(warped)\n",
    "        r_points = np.zeros_like(warped)\n",
    "        #get pixels for regression (currently unused)\n",
    "        nonzero=warped.nonzero()\n",
    "        nonzerox=np.array(nonzero[0])\n",
    "        nonzeroy=np.array(nonzero[1])\n",
    "        # Go through each level and draw the windows \n",
    "        for level in range(0,len(window_centroids)):\n",
    "            # Window_mask is a function to draw window areas\n",
    "            l_mask = window_mask(window_width,window_height,warped,window_centroids[level][0],level)\n",
    "            r_mask = window_mask(window_width,window_height,warped,window_centroids[level][1],level)\n",
    "            # Add graphic points from window mask here to total pixels found \n",
    "            l_points[(l_points == 255) | ((l_mask == 1) ) ] = 255\n",
    "            r_points[(r_points == 255) | ((r_mask == 1) ) ] = 255\n",
    "        # Draw windows        \n",
    "        template = np.array(r_points+l_points,np.uint8) # add both left and right window pixels together\n",
    "        zero_channel = np.zeros_like(template) # create a zero color channle \n",
    "        template = np.array(cv2.merge((zero_channel,template,zero_channel)),np.uint8) # make window pixels green\n",
    "        warpage = np.array(cv2.merge((255*warped,255*warped,255*warped)),np.uint8) # making the original road pixels 3 color channels\n",
    "        output = cv2.addWeighted(warpage, 1, template, 0.5, 0.0) # overlay the orignal road image with window results\n",
    "        #Draw polinomial lines\n",
    "        ploty =np.linspace(0, warped.shape[0]-1, warped.shape[0])\n",
    "        left_fitx=(left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2])\n",
    "        right_fitx = (right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2])\n",
    "        left_line=[]\n",
    "        right_line=[]\n",
    "        for y,l,r in zip(ploty, left_fitx, right_fitx):\n",
    "            left_line.append([np.int32(l), np.int32(y)])\n",
    "            right_line.append([np.int32(r), np.int32(y)])\n",
    "        pts1=np.array(left_line, np.int32)\n",
    "        pts2=np.array(right_line, np.int32)\n",
    "        pts1.reshape((-1,1,2))\n",
    "        pts2.reshape((-1,1,2))\n",
    "        output=cv2.polylines(output, [pts1, pts2], False, (255,0,0), 3)\n",
    "    # If no window centers found, just display orginal road image and empty polynom\n",
    "    else:\n",
    "        output = np.array(cv2.merge((warped,warped,warped)),np.uint8)\n",
    "        right_fit=[]\n",
    "        left_fit=[]\n",
    "    return output, left_fit, right_fit\n",
    "\n",
    "def tangent_circle(poly_l, poly_r, pic_center, eval_point=720):\n",
    "    \"\"\"Takes polynomial regressions and lanes center, returns curvature and distance to center [m]\"\"\"\n",
    "    #pixels to meter ratio\n",
    "    ym_per_pix = 3.0/106\n",
    "    xm_per_pix = 3.7/650\n",
    "    ploty = np.linspace(0, 719, num=720)\n",
    "    leftx=np.array([poly_l[0]*(y**2)+poly_l[1]*(y)+poly_l[0] for y in ploty])\n",
    "    rightx=np.array([poly_r[0]*(y**2)+poly_r[1]*(y)+poly_r[0] for y in ploty])\n",
    "    \n",
    "    center=(pic_center-700)*xm_per_pix\n",
    "\n",
    "    poly_ml=np.polyfit(ploty*ym_per_pix, leftx*xm_per_pix, 2)\n",
    "    poly_mr=np.polyfit(ploty*ym_per_pix, rightx*xm_per_pix, 2)\n",
    "    \n",
    "    eval_point*=ym_per_pix\n",
    "    \n",
    "    curv_l = ((1 + (2*poly_ml[0]*eval_point + poly_ml[1])**2)**1.5) / np.absolute(2*poly_ml[0])\n",
    "    curv_r = ((1 + (2*poly_mr[0]*eval_point + poly_mr[1])**2)**1.5) / np.absolute(2*poly_mr[0])\n",
    "    \n",
    "    return curv_l, curv_r, center\n",
    "\n",
    "def unwarp_color(warped_color, left_fit, right_fit, text, src=dst_pts, dst=src_pts):\n",
    "    \"\"\"Takes color image, returns identified poly, unwarps to original\"\"\"\n",
    "    #Draw polinomial lines\n",
    "    ploty =np.linspace(0, warped_color.shape[0]-1, warped_color.shape[0])\n",
    "    left_fitx=(left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2])\n",
    "    right_fitx = (right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2])\n",
    "    left_line=[]\n",
    "    right_line=[]\n",
    "    for y,l,r in zip(ploty, left_fitx, right_fitx):\n",
    "        left_line.append([np.int32(l), np.int32(y)])\n",
    "        right_line.append([np.int32(r), np.int32(y)])\n",
    "    pts1=np.array(left_line, np.int32)\n",
    "    pts2=np.array(right_line, np.int32)\n",
    "    pts1.reshape((-1,1,2))\n",
    "    pts2.reshape((-1,1,2))\n",
    "    total_pts=[]\n",
    "    total_pts.append(pts1)\n",
    "    total_pts.append(list(reversed(pts2)))\n",
    "    total_pts=np.array(total_pts).reshape((-1,1,2))\n",
    "    curvature_txt=\"Curvature: \"+str(text[0])[:5] +\"[m]\"\n",
    "    center_txt=\"Distance to lane Center: \"+str(text[2])[0:5]+\"[m]\"\n",
    "    output=np.zeros_like(warped_color)\n",
    "    output=cv2.fillPoly(output, [total_pts], color=(255,0,255))\n",
    "    output=cv2.polylines(output, [pts1, pts2], False, (255,0,0), 20)\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    output = cv2.warpPerspective(output, M, (warped_color.shape[1], warped_color.shape[0]), flags=cv2.INTER_NEAREST)\n",
    "    output = cv2.putText(output,curvature_txt, (50,50), cv2.FONT_HERSHEY_COMPLEX, fontScale=1, color=(255,255,255))\n",
    "    output = cv2.putText(output,center_txt, (50,100), cv2.FONT_HERSHEY_COMPLEX, fontScale=1, color=(255,255,255))\n",
    "    return output\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Full pipeline\n",
    "def sanity():\n",
    "    sane=True\n",
    "    return sane\n",
    "\n",
    "def pipeline(camera_image, window_width=window_width, window_height=window_height, margin=margin, save_step=False, save_dir=r'output_images/pipeline/', prefix='0'):\n",
    "    image, S_channel=undistort_s(camera_image)\n",
    "    gray=cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    gray=cv2.blur(gray,(3,3))\n",
    "    bin_im=multiple_threshold(gray, S_channel)\n",
    "    warp= birds_eye(bin_im)\n",
    "    warp_color=birds_eye(image)\n",
    "    window_centroids, pic_center = find_window_centroids(warp, window_width, window_height, margin)\n",
    "    detect_lines, left_fit, right_fit =draw_regression(warp, window_centroids, window_height)\n",
    "    curv_l, curv_r, center = tangent_circle(left_fit, right_fit, pic_center)\n",
    "    text_array=[curv_l, curv_r, center]\n",
    "    detect_color=unwarp_color(warp_color, left_fit, right_fit, text_array)\n",
    "    result=cv2.addWeighted(image, 1, detect_color, .8, 0)\n",
    "    if save_step:\n",
    "        cv2.imwrite(save_dir+prefix+\"undist.jpg\", image)\n",
    "        cv2.imwrite(save_dir+prefix+\"gray.jpg\", gray)\n",
    "        cv2.imwrite(save_dir+prefix+\"binary.jpg\", 255*bin_im)\n",
    "        cv2.imwrite(save_dir+prefix+\"S_channel.jpg\", 255*S_channel)\n",
    "        cv2.imwrite(save_dir+prefix+\"warp_color.jpg\", warp_color)\n",
    "        cv2.imwrite(save_dir+prefix+\"warp.jpg\", 255*warp)\n",
    "        cv2.imwrite(save_dir+prefix+\"detected.jpg\", detect_lines)\n",
    "        cv2.imwrite(save_dir+prefix+\"detected_color.jpg\", detect_color)\n",
    "        cv2.imwrite(save_dir+prefix+\"result.jpg\", result)\n",
    "    return result, left_fit, right_fit, text_array\n",
    "\n",
    "def frame_only(camera_image):\n",
    "    img, left_fit, right_fit, text_array = pipeline(camera_image)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "img_files = glob.glob('test_images/*.jpg')\n",
    "history=[]\n",
    "for i, file in enumerate(img_files):\n",
    "    camera_image=cv2.imread(file)\n",
    "    history.append(pipeline(camera_image, save_step=True, prefix=str(i))[1:])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video Test.mp4\n",
      "[MoviePy] Writing video Test.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████▉| 1260/1261 [06:21<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: Test.mp4 \n",
      "\n",
      "Wall time: 6min 22s\n"
     ]
    }
   ],
   "source": [
    "#for i in history:\n",
    "#    print (i)\n",
    "\n",
    "#Video\n",
    "from moviepy.editor import VideoFileClip\n",
    "#from IPython.display import HTML\n",
    "\n",
    "video_output=\"Test.mp4\"\n",
    "clip1=VideoFileClip(\"project_video.mp4\")\n",
    "video_clip=clip1.fl_image(frame_only)\n",
    "%time video_clip.write_videofile(video_output, audio=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
